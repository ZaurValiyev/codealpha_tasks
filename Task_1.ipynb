{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cc05cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                            f1_score, roc_auc_score, confusion_matrix, \n",
    "                            classification_report, roc_curve, auc)\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3641735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Loading and Preparation\n",
    "def load_data():\n",
    "    \"\"\"Load and prepare the German Credit dataset\"\"\"\n",
    "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\"\n",
    "    column_names = [\n",
    "        'checking_account', 'duration', 'credit_history', 'purpose', 'credit_amount',\n",
    "        'savings_account', 'employment', 'installment_rate', 'personal_status',\n",
    "        'other_debtors', 'residence_since', 'property', 'age', 'other_installment_plans',\n",
    "        'housing', 'existing_credits', 'job', 'dependents', 'telephone', 'foreign_worker', 'credit_risk'\n",
    "    ]\n",
    "    \n",
    "    data = pd.read_csv(url, delimiter=' ', header=None, names=column_names)\n",
    "    data['credit_risk'] = data['credit_risk'].replace({1: 1, 2: 0})  # 1=Good, 0=Bad\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50f7d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Preprocessing Setup\n",
    "def get_preprocessor():\n",
    "    \"\"\"Create preprocessing pipeline for numeric and categorical features\"\"\"\n",
    "    categorical_features = ['checking_account', 'credit_history', 'purpose', \n",
    "                          'savings_account', 'employment', 'personal_status',\n",
    "                          'other_debtors', 'property', 'other_installment_plans',\n",
    "                          'housing', 'job', 'telephone', 'foreign_worker']\n",
    "    numerical_features = ['duration', 'credit_amount', 'installment_rate',\n",
    "                         'residence_since', 'age', 'existing_credits', 'dependents']\n",
    "    \n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())])\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numerical_features),\n",
    "            ('cat', categorical_transformer, categorical_features)])\n",
    "    \n",
    "    return preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1692ca03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Model Training with Hyperparameter Tuning\n",
    "def train_random_forest(X_train, y_train, preprocessor):\n",
    "    \"\"\"Train and tune a Random Forest classifier\"\"\"\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', RandomForestClassifier(random_state=42, class_weight='balanced'))\n",
    "    ])\n",
    "    \n",
    "    param_grid = {\n",
    "        'classifier__n_estimators': [100, 200],\n",
    "        'classifier__max_depth': [None, 10, 20],\n",
    "        'classifier__min_samples_split': [2, 5],\n",
    "        'classifier__min_samples_leaf': [1, 2],\n",
    "        'classifier__max_features': ['sqrt']\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='roc_auc', n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    return grid_search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fd23aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Model Evaluation\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Evaluate model performance and plot ROC curve\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1 Score': f1_score(y_test, y_pred),\n",
    "        'ROC AUC': roc_auc_score(y_test, y_proba)\n",
    "    }\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"\\nModel Performance:\")\n",
    "    for name, value in metrics.items():\n",
    "        print(f\"{name}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69834f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Feature Importance Analysis\n",
    "def plot_feature_importance(model, preprocessor):\n",
    "    \"\"\"Plot feature importances from the trained model\"\"\"\n",
    "    # Get feature names\n",
    "    numeric_features = preprocessor.transformers_[0][2]\n",
    "    categorical_features = preprocessor.transformers_[1][2]\n",
    "    \n",
    "    # Get categorical feature names\n",
    "    categorical_transformer = preprocessor.transformers_[1][1]\n",
    "    if hasattr(categorical_transformer, 'named_steps'):\n",
    "        onehot = categorical_transformer.named_steps['onehot']\n",
    "        cat_feature_names = onehot.get_feature_names_out(categorical_features)\n",
    "    else:\n",
    "        cat_feature_names = categorical_transformer.get_feature_names_out(categorical_features)\n",
    "    \n",
    "    all_feature_names = numeric_features + list(cat_feature_names)\n",
    "    \n",
    "    # Get feature importances\n",
    "    importances = model.named_steps['classifier'].feature_importances_\n",
    "    \n",
    "    # Create DataFrame and sort\n",
    "    feature_importances = pd.DataFrame({'feature': all_feature_names, 'importance': importances})\n",
    "    feature_importances = feature_importances.sort_values('importance', ascending=False).head(20)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_importances['feature'], feature_importances['importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Top 20 Feature Importances')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634d6445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Model Saving and Prediction Example\n",
    "def save_and_predict(model, preprocessor):\n",
    "    \"\"\"Save model and demonstrate prediction\"\"\"\n",
    "    # Save the model\n",
    "    joblib.dump(model, 'random_forest_credit_model.pkl')\n",
    "    print(\"\\nModel saved as 'random_forest_credit_model.pkl'\")\n",
    "    \n",
    "    # Create sample data\n",
    "    sample_data = {\n",
    "        'checking_account': 'A11',\n",
    "        'duration': 24,\n",
    "        'credit_history': 'A32',\n",
    "        'purpose': 'A43',\n",
    "        'credit_amount': 5000,\n",
    "        'savings_account': 'A61',\n",
    "        'employment': 'A73',\n",
    "        'installment_rate': 4,\n",
    "        'personal_status': 'A93',\n",
    "        'other_debtors': 'A101',\n",
    "        'residence_since': 4,\n",
    "        'property': 'A121',\n",
    "        'age': 35,\n",
    "        'other_installment_plans': 'A143',\n",
    "        'housing': 'A152',\n",
    "        'existing_credits': 2,\n",
    "        'job': 'A173',\n",
    "        'dependents': 1,\n",
    "        'telephone': 'A192',\n",
    "        'foreign_worker': 'A201'\n",
    "    }\n",
    "    \n",
    "    sample_df = pd.DataFrame([sample_data])\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(sample_df)\n",
    "    probability = model.predict_proba(sample_df)[:, 1]\n",
    "    \n",
    "    print(\"\\nSample Prediction:\")\n",
    "    print(f\"Predicted Credit Risk: {'Good' if prediction[0] == 1 else 'Bad'}\")\n",
    "    print(f\"Probability of Good Credit: {probability[0]:.2%}\")\n",
    "\n",
    "# Main Execution\n",
    "def main():\n",
    "    print(\"1. Loading data...\")\n",
    "    data = load_data()\n",
    "    X = data.drop('credit_risk', axis=1)\n",
    "    y = data['credit_risk']\n",
    "    \n",
    "    print(\"\\n2. Splitting data into train/test sets...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    print(\"\\n3. Setting up preprocessing...\")\n",
    "    preprocessor = get_preprocessor()\n",
    "    \n",
    "    print(\"\\n4. Training Random Forest model with hyperparameter tuning...\")\n",
    "    grid_search = train_random_forest(X_train, y_train, preprocessor)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    print(\"\\n5. Evaluating model performance...\")\n",
    "    metrics = evaluate_model(best_model, X_test, y_test)\n",
    "    \n",
    "    print(\"\\n6. Analyzing feature importance...\")\n",
    "    plot_feature_importance(best_model, preprocessor)\n",
    "    \n",
    "    print(\"\\n7. Saving model and testing prediction...\")\n",
    "    save_and_predict(best_model, preprocessor)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
